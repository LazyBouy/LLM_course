{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxB6dAb3nx6s"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Word variations in Claude tokenizer<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jASJ6qhCZYG2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/python/nlp/LLM_MC/LLM_course/.env_LLM/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/claude-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48RA3TSLZZTD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uiG7eQzcEFg"
   },
   "source": [
    "# Exploring the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bkYuR2svaL3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EuUYPzxUaL0j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3490, 365, 895, 1373]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('this is some text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JfwdmW5PaYwS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3490, 365, 895, 1373], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('this is some text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_ZhDYMSVaLxr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3490,  365,  895, 1373]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('this is some text',return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADTrqWtwafGn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Md7b5Ewae_2"
   },
   "source": [
    "# Some examples of multitoken words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U2YaY0IAZZQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hypothetical\" has 2 tokens ([30678, 36881]):\n",
      "   ['hypot', 'hetical']\n",
      "\n",
      "\" hypothetical\" has 1 tokens ([44086]):\n",
      "   [' hypothetical']\n",
      "\n",
      "\"hypothetical \" has 3 tokens ([30678, 36881, 225]):\n",
      "   ['hypot', 'hetical', ' ']\n",
      "\n",
      "\" hypothteical\" has 4 tokens ([18806, 1309, 384, 709]):\n",
      "   [' hyp', 'oth', 'te', 'ical']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [ 'hypothetical',\n",
    "          ' hypothetical',\n",
    "          'hypothetical ',\n",
    "          ' hypothteical'  ]\n",
    "\n",
    "for word in words:\n",
    "  toks = tokenizer.encode(word)\n",
    "  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "92n9vHOMZZNE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"can't\" has 2 tokens ([3152, 828]):\n",
      "   ['can', \"'t\"]\n",
      "\n",
      "\" can't\" has 2 tokens ([689, 828]):\n",
      "   [' can', \"'t\"]\n",
      "\n",
      "\"cant\" has 1 tokens ([25918]):\n",
      "   ['cant']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [ \"can't\",\n",
    "          \" can't\",\n",
    "          \"cant\" ]\n",
    "\n",
    "for word in words:\n",
    "  toks = tokenizer.encode(word)\n",
    "  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G94YpqWIZZKN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" straight forward\" has 2 tokens ([7638, 4258]):\n",
      "   [' straight', ' forward']\n",
      "\n",
      "\" straightforward\" has 1 tokens ([29388]):\n",
      "   [' straightforward']\n",
      "\n",
      "\" straight-forward\" has 3 tokens ([7638, 17, 6978]):\n",
      "   [' straight', '-', 'forward']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [ ' straight forward',\n",
    "          ' straightforward',\n",
    "          ' straight-forward'  ]\n",
    "\n",
    "for word in words:\n",
    "  toks = tokenizer.encode(word)\n",
    "  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VlEjISXDZZHH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"parttime\" has 2 tokens ([1620, 793]):\n",
      "   ['part', 'time']\n",
      "\n",
      "\" part-time\" has 3 tokens ([919, 17, 793]):\n",
      "   [' part', '-', 'time']\n",
      "\n",
      "\"part-time\" has 3 tokens ([1620, 17, 793]):\n",
      "   ['part', '-', 'time']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [ 'parttime',\n",
    "          ' part-time',\n",
    "          'part-time' ]\n",
    "\n",
    "for word in words:\n",
    "  toks = tokenizer.encode(word)\n",
    "  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6AKcZX7ZZEj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NIbDxr3ZZBx"
   },
   "source": [
    "# Common misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hNSgpVEzZY-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" accommodate acommodate\" has 4 tokens ([27029, 1163, 17293, 353]):\n",
      "   [' accommodate', ' ac', 'ommod', 'ate']\n",
      "\n",
      "\" a lot alot\" has 4 tokens ([269, 3949, 460, 331]):\n",
      "   [' a', ' lot', ' al', 'ot']\n",
      "\n",
      "\" definately definitely\" has 3 tokens ([39887, 2704, 14159]):\n",
      "   [' defin', 'ately', ' definitely']\n",
      "\n",
      "\"occurrence occurence occurance\" has 6 tokens ([48275, 2535, 540, 11970, 8265, 634]):\n",
      "   ['occurrence', ' occ', 'ure', 'nce', ' occur', 'ance']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [ ' accommodate acommodate',\n",
    "          ' a lot alot',\n",
    "          ' definately definitely',\n",
    "          'occurrence occurence occurance']\n",
    "\n",
    "for word in words:\n",
    "  toks = tokenizer.encode(word)\n",
    "  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Gtu-vdCdGF2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydb8aKwVdGDZ"
   },
   "source": [
    "# Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "isfwgyfBdGAq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"People say, you shouldn't do that.\" has 9 tokens ([9308, 2236, 16, 583, 12329, 828, 705, 427, 18]):\n",
      "   ['People', ' say', ',', ' you', ' shouldn', \"'t\", ' do', ' that', '.']\n",
      "\n",
      "\"People say; you shouldn't do that.\" has 9 tokens ([9308, 2236, 31, 583, 12329, 828, 705, 427, 18]):\n",
      "   ['People', ' say', ';', ' you', ' shouldn', \"'t\", ' do', ' that', '.']\n",
      "\n",
      "\"People say -- you shouldn't do that.\" has 9 tokens ([9308, 2236, 1786, 583, 12329, 828, 705, 427, 18]):\n",
      "   ['People', ' say', ' --', ' you', ' shouldn', \"'t\", ' do', ' that', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [ \"People say, you shouldn't do that.\",\n",
    "              \"People say; you shouldn't do that.\",\n",
    "              \"People say -- you shouldn't do that.\",  ]\n",
    "\n",
    "for senten in sentences:\n",
    "  toks = tokenizer.encode(senten)\n",
    "  print(f'\"{senten}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_1hnloRddF9u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"“Curiouser and curiouser!” cried Alice (she was so much surprised, that for the moment she quite forgot how to speak good English).\" has 32 tokens ([3675, 39, 4427, 304, 400, 329, 1803, 21293, 400, 25912, 16905, 32268, 344, 3647, 504, 779, 1935, 14843, 16, 427, 351, 279, 4152, 970, 5055, 14759, 1369, 317, 6006, 1929, 6075, 673]):\n",
      "   ['“', 'C', 'uri', 'ou', 'ser', ' and', ' cur', 'iou', 'ser', '!”', ' cried', ' Alice', ' (', 'she', ' was', ' so', ' much', ' surprised', ',', ' that', ' for', ' the', ' moment', ' she', ' quite', ' forgot', ' how', ' to', ' speak', ' good', ' English', ').']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '“Curiouser and curiouser!” cried Alice (she was so much surprised, that for the moment she quite forgot how to speak good English).'\n",
    "\n",
    "toks = tokenizer.encode(text)\n",
    "print(f'\"{text}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ECXuCbPxZY8E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"“No, no!” said the Queen. “Sentence first—verdict afterwards.”\" has 17 tokens ([3675, 2364, 16, 841, 25912, 1146, 279, 15146, 18, 1594, 40268, 1082, 1572, 366, 851, 20563, 3201]):\n",
      "   ['“', 'No', ',', ' no', '!”', ' said', ' the', ' Queen', '.', ' “', 'Sentence', ' first', '—', 'ver', 'dict', ' afterwards', '.”']\n",
      "\n",
      "\"“No, no!” said the Queen. “Sentence first - verdict afterwards.”\" has 16 tokens ([3675, 2364, 16, 841, 25912, 1146, 279, 15146, 18, 1594, 40268, 1082, 464, 25402, 20563, 3201]):\n",
      "   ['“', 'No', ',', ' no', '!”', ' said', ' the', ' Queen', '.', ' “', 'Sentence', ' first', ' -', ' verdict', ' afterwards', '.”']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [ '“No, no!” said the Queen. “Sentence first—verdict afterwards.”',\n",
    "              '“No, no!” said the Queen. “Sentence first - verdict afterwards.”']\n",
    "\n",
    "for senten in sentences:\n",
    "  toks = tokenizer.encode(senten)\n",
    "  print(f'\"{senten}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VagisaHOemjL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4KcsfaCe1KY"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GHi-Nx91e1H6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\textbf{Leibniz} & $\\frac{dy}{dx}$\" has 16 tokens ([64, 57619, 95, 2447, 606, 82, 461, 97, 1273, 1982, 3498, 95, 5413, 3478, 7844, 2084]):\n",
      "   ['\\\\', 'textbf', '{', 'Le', 'ib', 'n', 'iz', '}', ' &', ' $\\\\', 'frac', '{', 'dy', '}{', 'dx', '}$']\n",
      "\n",
      "ight]\" has 68 tokens ([64977, 669, 3961, 1151, 76, 202, 83, 20, 97, 200, 80, 2031, 669, 1903, 63, 205, 12460, 95, 74, 12, 92, 15, 76, 4547, 74, 12, 92, 27681, 76, 97, 206, 510, 65, 57037, 7372, 31, 669, 3961, 1151, 76, 202, 83, 20, 97, 225, 201, 1767, 669, 1903, 63, 205, 12460, 95, 75, 12, 92, 15, 76, 4547, 75, 12, 92, 27681, 76, 97, 206, 510, 65]):\n",
      "   ['&=', ' \\\\', 'lim', '_{', 'h', '\\t', 'o', '0', '}', '\\x07', 'l', 'pha', ' \\\\', 'left', '[', '\\x0c', 'rac', '{', 'f', '(', 'x', '+', 'h', ')-', 'f', '(', 'x', ')}{', 'h', '}', '\\r', 'ight', ']', ' \\\\;', '+\\\\', ';', ' \\\\', 'lim', '_{', 'h', '\\t', 'o', '0', '}', ' ', '\\x08', 'eta', ' \\\\', 'left', '[', '\\x0c', 'rac', '{', 'g', '(', 'x', '+', 'h', ')-', 'g', '(', 'x', ')}{', 'h', '}', '\\r', 'ight', ']']\n",
      "\n",
      "\"diffVects = targetActs[layeri,:,:,1] - targetActs[layeri-1,:,:,1]; diffNorms[layeri,1] = np.linalg.norm(diffVects,axis=1).mean()\" has 53 tokens ([4247, 58, 458, 87, 284, 1776, 55619, 63, 3460, 77, 17313, 6576, 21, 65, 464, 1776, 55619, 63, 3460, 77, 17, 21, 17313, 6576, 21, 5363, 3079, 9815, 87, 63, 3460, 77, 16, 21, 65, 284, 892, 18, 11673, 18, 4649, 12, 4247, 58, 458, 87, 16, 3159, 33, 21, 673, 2659, 370]):\n",
      "   ['diff', 'V', 'ect', 's', ' =', ' target', 'Acts', '[', 'layer', 'i', ',:,', ':,', '1', ']', ' -', ' target', 'Acts', '[', 'layer', 'i', '-', '1', ',:,', ':,', '1', '];', ' diff', 'Norm', 's', '[', 'layer', 'i', ',', '1', ']', ' =', ' np', '.', 'linalg', '.', 'norm', '(', 'diff', 'V', 'ect', 's', ',', 'axis', '=', '1', ').', 'mean', '()']\n",
      "\n",
      "\"[Y,X] = meshgrid(linspace(-4,4,21)); G = exp( -(X.^2+Y.^2)/10 );\" has 34 tokens ([63, 61, 16, 60, 65, 284, 12443, 2340, 12, 12509, 2009, 24, 16, 24, 16, 1999, 9186, 516, 284, 1848, 12, 31941, 60, 18, 66, 22, 15, 61, 18, 66, 22, 2787, 749, 13160]):\n",
      "   ['[', 'Y', ',', 'X', ']', ' =', ' mesh', 'grid', '(', 'linspace', '(-', '4', ',', '4', ',', '21', '));', ' G', ' =', ' exp', '(', ' -(', 'X', '.', '^', '2', '+', 'Y', '.', '^', '2', ')/', '10', ' );']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/tmp/ipykernel_95414/1058427079.py:2: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  '&= \\lim_{h\\to0}\\alpha \\left[\\frac{f(x+h)-f(x)}{h}\\right] \\;+\\; \\lim_{h\\to0} \\beta \\left[\\frac{g(x+h)-g(x)}{h}\\right]', # latex\n"
     ]
    }
   ],
   "source": [
    "sentences = [ '\\\\textbf{Leibniz} & $\\\\frac{dy}{dx}$', # latex\n",
    "              '&= \\lim_{h\\to0}\\alpha \\left[\\frac{f(x+h)-f(x)}{h}\\right] \\;+\\; \\lim_{h\\to0} \\beta \\left[\\frac{g(x+h)-g(x)}{h}\\right]', # latex\n",
    "              'diffVects = targetActs[layeri,:,:,1] - targetActs[layeri-1,:,:,1]; diffNorms[layeri,1] = np.linalg.norm(diffVects,axis=1).mean()', # python\n",
    "              '[Y,X] = meshgrid(linspace(-4,4,21)); G = exp( -(X.^2+Y.^2)/10 );' # MATLAB\n",
    "              ]\n",
    "\n",
    "for senten in sentences:\n",
    "  toks = tokenizer.encode(senten)\n",
    "  print(f'\"{senten}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buiTri6se1AO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUFPQMR5Xu6joIiBF59cEW",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env_LLM",
   "language": "python",
   "name": ".env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
