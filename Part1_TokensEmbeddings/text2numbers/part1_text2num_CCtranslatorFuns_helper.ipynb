{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPXS7dDYcImb"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: More on token translation<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqTos9Wpuq1-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XnNp0fLuqyh"
   },
   "source": [
    "# Import two tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QnBjS3LAujoq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/python/nlp/LLM_MC/LLM_course/.env_LLM/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install tiktoken\n",
    "import tiktoken\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wJfxECXYusgP"
   },
   "outputs": [],
   "source": [
    "bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt4Tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7DbSkSquqp5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur-eP9j7uqm8"
   },
   "source": [
    "# Exercise 1: Write translation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PjXLN6TTwcQ6"
   },
   "outputs": [],
   "source": [
    "# translation functions\n",
    "def bert2gpt4(bertToks):\n",
    "  b = bertTokenizer.decode(bertToks) # decode\n",
    "  g = gpt4Tokenizer.encode(b) # encode\n",
    "  return g\n",
    "\n",
    "def gpt42bert(gpt4Toks):\n",
    "  g = gpt4Tokenizer.decode(gpt4Toks) # decode\n",
    "  b = bertTokenizer.encode(g) # encode\n",
    "  return b[1:-1] # bert auto-adds [CLS] ... [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MXpmFE3cBfPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 88816, 60, 602, 6562, 18414, 1051, 25977, 13, 510, 82476, 60]\n",
      "[1045, 4299, 7967, 2020, 6379, 1012]\n"
     ]
    }
   ],
   "source": [
    "# just checking that it gives no errors\n",
    "text = 'I wish chocolate were purple.'\n",
    "\n",
    "print(bert2gpt4(bertTokenizer.encode(text)))\n",
    "print(gpt42bert(gpt4Tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txY-ShywAJ_S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4Du3JPXAJ6p"
   },
   "source": [
    "# Exercise 2: BERT --> GPT4 --> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lkf2RFVk76xH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "  I wanted to paste in a thought-provoking quote here, but I didn't.\n",
      "\n",
      "BERT tokens:\n",
      "  [101, 1045, 2359, 2000, 19351, 1999, 1037, 2245, 1011, 4013, 22776, 14686, 2182, 1010, 2021, 1045, 2134, 1005, 1056, 1012, 102]\n",
      "\n",
      "BERT to GPT4:\n",
      "  [CLS] i wanted to paste in a thought - provoking quote here, but i didn't. [SEP]\n",
      "\n",
      "Back to BERT:\n",
      "  [CLS] i wanted to paste in a thought - provoking quote here, but i didn't. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"I wanted to paste in a thought-provoking quote here, but I didn't.\"\n",
    "print(f'Original text\\n  {text}\\n')\n",
    "\n",
    "# initial encoding\n",
    "bertTox = bertTokenizer.encode(text)\n",
    "print(f'BERT tokens:\\n  {bertTox}\\n')\n",
    "\n",
    "# translate to GPT4\n",
    "b2g = bert2gpt4(bertTox)\n",
    "print(f'BERT to GPT4:\\n  {gpt4Tokenizer.decode(b2g)}\\n')\n",
    "\n",
    "# back-translate to BERT\n",
    "back2bert = gpt42bert(b2g)\n",
    "print(f'Back to BERT:\\n  {bertTokenizer.decode(back2bert)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WumOSErA5pJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipwIs91jA5mB"
   },
   "source": [
    "# Exercise 3: GPT4 --> BERT --> GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G1bpWEnT33Es"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "  I still don't have a good quote here. Now it's too late.\n",
      "\n",
      "GPT4 tokens:\n",
      "  [40, 2103, 1541, 956, 617, 264, 1695, 12929, 1618, 13, 4800, 433, 596, 2288, 3389, 13]\n",
      "\n",
      "GPT4 to BERT:\n",
      "  i still don't have a good quote here. now it's too late.\n",
      "BERT to GPT4:\n",
      "  i still don't have a good quote here. now it's too late.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"I still don't have a good quote here. Now it's too late.\"\n",
    "print(f'Original text\\n  {text}\\n')\n",
    "\n",
    "# initial encoding\n",
    "gpt4Tox = gpt4Tokenizer.encode(text)\n",
    "print(f'GPT4 tokens:\\n  {gpt4Tox}\\n')\n",
    "\n",
    "# translate to BERT\n",
    "g2b = gpt42bert(gpt4Tox)\n",
    "print(f'GPT4 to BERT:\\n  {bertTokenizer.decode(g2b)}')\n",
    "\n",
    "\n",
    "# back-translate to GPT4\n",
    "back2gpt4 = bert2gpt4(g2b)\n",
    "print(f'BERT to GPT4:\\n  {gpt4Tokenizer.decode(back2gpt4)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "309X7W4O33Bi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+3GYaK7YmfsINQR8Q23GI",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env_LLM",
   "language": "python",
   "name": ".env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
