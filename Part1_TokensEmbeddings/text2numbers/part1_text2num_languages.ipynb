{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnNdFahh7-8z"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Tokenization in different languages<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O-rylt4G8MWp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/python/nlp/LLM_MC/LLM_course/.env_LLM/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# need to install the tiktoken library to get OpenAI's tokenizer\n",
    "#!pip install tiktoken\n",
    "import tiktoken\n",
    "tokenizerG = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizerB = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vll_zH5t_gHC"
   },
   "outputs": [],
   "source": [
    "languages = ['English','Spanish','Arabic','Persian','Lithuanian','Chinese','Tamil','Esperanto']\n",
    "\n",
    "sentences = [ 'Blue towels are great because they remind you of the sea, although the sea is wet and towels work better when they are dry.',\n",
    "              'Las toallas azules son geniales porque recuerdan al mar, aunque el mar está mojado y las toallas funcionan mejor cuando están secas.',\n",
    "              'تعتبر المناشف الزرقاء رائعة لأنها تذكرك بالبحر، على الرغم من أن البحر مبلل والمناشف تعمل بشكل أفضل عندما تكون جافة.',\n",
    "              'حوله‌های آبی عالی هستند زیرا شما را به یاد دریا می‌اندازند، اگرچه دریا مرطوب است و حوله‌ها وقتی خشک باشند بهتر عمل می‌کنند.',\n",
    "              'Mėlyni rankšluosčiai puikūs, nes primena jūrą, nors jūra yra šlapia, o rankšluosčiai geriau tinka, kai yra sausi.',\n",
    "              '蓝色毛巾很棒，因为它们会让您想起大海，尽管海水是湿的，而毛巾在干燥时效果更好。',\n",
    "              'நீல நிற துண்டுகள் சிறந்தவை, ஏனென்றால் அவை கடலை நினைவூட்டுகின்றன, இருப்பினும் கடல் ஈரமாக இருக்கும், துண்டுகள் உலர்ந்திருக்கும் போது சிறப்பாக வேலை செய்யும்.',\n",
    "              'Bluaj mantukoj estas bonegaj ĉar ili memorigas vin pri la maro, kvankam la maro estas malseka kaj mantukoj funkcias pli bone kiam ili estas sekaj.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f34QAbA-_gD-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Language  |  Chars  |  BERT  |  GPT \n",
      "-------------------------------------\n",
      "   English |   123   |   26   |   26\n",
      "   Spanish |   132   |   46   |   34\n",
      "    Arabic |   115   |   95   |   84\n",
      "   Persian |   123   |   96   |   90\n",
      "Lithuanian |   113   |   46   |   57\n",
      "   Chinese |    39   |   39   |   55\n",
      "     Tamil |   154   |   35   |  209\n",
      " Esperanto |   146   |   56   |   50\n"
     ]
    }
   ],
   "source": [
    "# table header\n",
    "print(' Language  |  Chars  |  BERT  |  GPT ')\n",
    "print('-'*37)\n",
    "\n",
    "for lang,text in zip(languages,sentences):\n",
    "\n",
    "  # tokenize the text\n",
    "  tokensG = tokenizerG.encode(text)\n",
    "  tokensB = tokenizerB.encode(text)[1:-1]\n",
    "\n",
    "  # print the result\n",
    "  print(f'{lang:>10} |   {len(text):3}   |  {len(tokensB):3}   |  {len(tokensG):3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mUPMszdL5mM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCb6CIN0+JiQEhqiX4+TkK",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env_LLM",
   "language": "python",
   "name": ".env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
