{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMIX8VSYc2Tm"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Tokenization compression ratios<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GmDlHtoc2Or"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C8JzcByJ_dVt"
   },
   "outputs": [],
   "source": [
    "# for getting text data off the web\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# strings\n",
    "import string\n",
    "\n",
    "#!pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f34QAbA-_gD-"
   },
   "outputs": [],
   "source": [
    "# GPT-4's tokenizer\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vll_zH5t_gHC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsfMaYxen8oU"
   },
   "source": [
    "# Exercise 1: The books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fX_ZgpveSYPL"
   },
   "outputs": [],
   "source": [
    "# all books have the same url format;\n",
    "# they are unique by numerical code\n",
    "baseurl = 'https://www.gutenberg.org/cache/epub/'\n",
    "\n",
    "bookurls = [\n",
    "    # code       title\n",
    "    ['84',    'Frankenstein'    ],\n",
    "    ['64317', 'GreatGatsby'     ],\n",
    "    ['11',    'AliceWonderland' ],\n",
    "    ['1513',  'RomeoJuliet'     ],\n",
    "    ['76',    'HuckFinn'        ],\n",
    "    ['219',   'HeartDarkness'   ],\n",
    "    ['2591',  'GrimmsTales'     ],\n",
    "    ['2148',  'EdgarAllenPoe'   ],\n",
    "    ['36',    'WarOfTheWorlds'  ],\n",
    "    ['829',   'GulliversTravels']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AW2MzjfdRhB8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Book title     |  Chars  |  Tokens | Compression\n",
      "--------------------------------------------------\n",
      "Frankenstein     | 446,544 | 102,419 |  22.94%\n",
      "GreatGatsby      | 296,858 |  70,343 |  23.70%\n",
      "AliceWonderland  | 167,674 |  41,457 |  24.72%\n",
      "RomeoJuliet      | 167,429 |  43,761 |  26.14%\n",
      "HuckFinn         | 602,714 | 159,125 |  26.40%\n",
      "HeartDarkness    | 232,885 |  56,483 |  24.25%\n",
      "GrimmsTales      | 549,736 | 137,252 |  24.97%\n",
      "EdgarAllenPoe    | 632,136 | 144,315 |  22.83%\n",
      "WarOfTheWorlds   | 363,420 |  84,580 |  23.27%\n",
      "GulliversTravels | 611,742 | 143,560 |  23.47%\n"
     ]
    }
   ],
   "source": [
    "print('  Book title     |  Chars  |  Tokens | Compression')\n",
    "print('-'*50)\n",
    "\n",
    "for code,title in bookurls:\n",
    "\n",
    "    # get the text\n",
    "    fullurl = baseurl + code + '/pg' + code + '.txt'\n",
    "    text = requests.get(fullurl).text\n",
    "    num_chars = len(text)\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = tokenizer.encode(text)\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    # compression ratio\n",
    "    compress = 100 * num_tokens/num_chars\n",
    "    \n",
    "    print(f'{title:16} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G8IwDojRg_E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FZ6gF8zRg8H"
   },
   "source": [
    "# Exercise 2: Repeat with websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KOTAgq4PXMVc"
   },
   "outputs": [],
   "source": [
    "weburls = [\n",
    "    'http://python.org/',\n",
    "    'https://pytorch.org/',\n",
    "    'https://en.wikipedia.org/wiki/List_of_English_words_containing_Q_not_followed_by_U',\n",
    "    'https://sudoku.com/',\n",
    "    'https://reddit.com/',\n",
    "    'https://visiteurope.com/en/',\n",
    "    'https://sincxpress.com/',\n",
    "    'https://openai.com/',\n",
    "    'https://theuselessweb.com/',\n",
    "    'https://maps.google.com/',\n",
    "    'https://pigeonsarentreal.co.uk/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hC7ZulNxRg5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Website        |  Chars  |  Tokens | Compression\n",
      "-----------------------------------------------------\n",
      "python             |  49,940 |  12,697 |  25.42%\n",
      "pytorch            | 122,590 |  35,187 |  28.70%\n",
      "en.wikipedia       |     126 |      34 |  26.98%\n",
      "sudoku             | 141,238 |  51,460 |  36.43%\n",
      "reddit             | 440,745 | 140,934 |  31.98%\n",
      "visiteurope        | 402,220 | 154,476 |  38.41%\n",
      "sincxpress         |  25,580 |   6,843 |  26.75%\n",
      "openai             |  11,596 |   6,486 |  55.93%\n",
      "theuselessweb      |   4,756 |   1,329 |  27.94%\n",
      "maps.google        |  33,441 |  10,904 |  32.61%\n",
      "pigeonsarentreal.c |   7,304 |   2,055 |  28.14%\n"
     ]
    }
   ],
   "source": [
    "print('    Website        |  Chars  |  Tokens | Compression')\n",
    "print('-'*53)\n",
    "\n",
    "for url in weburls:\n",
    "\n",
    "  # get the text\n",
    "  text = requests.get(url).text\n",
    "  num_chars = len(text)\n",
    "\n",
    "  # tokenize\n",
    "  tokens = tokenizer.encode(text)\n",
    "  num_tokens = len(tokens)\n",
    "\n",
    "  # compression ratio\n",
    "  compress = 100 * num_tokens/num_chars\n",
    "\n",
    "  print(f'{urlparse(url).hostname[:-4]:18} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RczL4_fIWVBK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOShW7tBhftK"
   },
   "source": [
    "# Exercise 3: Using the 'string' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i_3jPwujPEpV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A collection of string constants.\\n\\nPublic module variables:\\n\\nwhitespace -- a string containing all ASCII whitespace\\nascii_lowercase -- a string containing all ASCII lowercase letters\\nascii_uppercase -- a string containing all ASCII uppercase letters\\nascii_letters -- a string containing all ASCII letters\\ndigits -- a string containing all ASCII decimal digits\\nhexdigits -- a string containing all ASCII hexadecimal digits\\noctdigits -- a string containing all ASCII octal digits\\npunctuation -- a string containing all ASCII punctuation characters\\nprintable -- a string containing all ASCII characters considered printable\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.,\n",
    "string.__dict__['__doc__']\n",
    "# string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pozOkYnVfwSI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attribute     |  Chars  |  Tokens | Compression\n",
      "--------------------------------------------------\n",
      "__name__        |       6 |       1 | 16.67\n",
      "__doc__         |     622 |     109 | 17.52\n",
      "__file__        |      77 |      24 | 31.17\n",
      "__cached__      |     102 |      33 | 32.35\n",
      "whitespace      |       6 |       4 | 66.67\n",
      "ascii_lowercase |      26 |       1 |  3.85\n",
      "ascii_uppercase |      26 |       1 |  3.85\n",
      "ascii_letters   |      52 |       2 |  3.85\n",
      "digits          |      10 |       4 | 40.00\n",
      "hexdigits       |      22 |       7 | 31.82\n",
      "octdigits       |       8 |       3 | 37.50\n",
      "punctuation     |      32 |      21 | 65.62\n",
      "printable       |     100 |      31 | 31.00\n"
     ]
    }
   ],
   "source": [
    "print('  Attribute     |  Chars  |  Tokens | Compression')\n",
    "print('-'*50)\n",
    "\n",
    "for k,v in string.__dict__.items():\n",
    "    if isinstance(v, str) and (len(v)>0):\n",
    "    \n",
    "        # get the text\n",
    "        num_chars = len(v)\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = tokenizer.encode(v)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        # compression ratio\n",
    "        compress = 100 * num_tokens/num_chars\n",
    "        \n",
    "        # print the results\n",
    "        print(f'{k:15} | {num_chars:>7,d} | {num_tokens:>7,d} | {compress:>5.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmF_zWxheB1F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPCu3ErzRpYhUHCAoMYJIgw",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env_LLM",
   "language": "python",
   "name": ".env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
