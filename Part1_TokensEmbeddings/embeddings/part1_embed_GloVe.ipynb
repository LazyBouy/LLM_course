{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhhbJ87Vy7yJ"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Pretrained embeddings (GloVe)<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /root/projects/python/nlp/LLM_MC/LLM_course/.env_LLM/lib/python3.13/site-packages (from gensim) (2.3.5)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Downloading gensim-4.4.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Installing collected packages: wrapt, scipy, smart_open, gensim\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [gensim]━━━━\u001b[0m \u001b[32m3/4\u001b[0m [gensim]pen]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.4.0 scipy-1.16.3 smart_open-7.5.0 wrapt-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Io0sCTA0WcfI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 35.5% 23.4/66.0MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.2% 60.2/66.0MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download a small GloVe model (Wikipedia + Gigaword, 50D)\n",
    "\n",
    "# NOTE: If you get errors importing, run the following !pip... line,\n",
    "# then restart your session (from Runtime menu) and comment out the pip line.\n",
    "# !pip install gensim\n",
    "\n",
    "import gensim.downloader as api\n",
    "glove = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KSKxAmbEWVEn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# svg plots\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa2CRvtAwZPa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAtIwgk0wZMh"
   },
   "source": [
    "# Explore the glove variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dPa6pXYiWccV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__firstlineno__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_log_evaluate_word_analogies',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " '_upconvert_old_d2vkv',\n",
       " '_upconvert_old_vocab',\n",
       " 'add_lifecycle_event',\n",
       " 'add_vector',\n",
       " 'add_vectors',\n",
       " 'allocate_vecattrs',\n",
       " 'closer_than',\n",
       " 'cosine_similarities',\n",
       " 'distance',\n",
       " 'distances',\n",
       " 'doesnt_match',\n",
       " 'evaluate_word_analogies',\n",
       " 'evaluate_word_pairs',\n",
       " 'expandos',\n",
       " 'fill_norms',\n",
       " 'get_index',\n",
       " 'get_mean_vector',\n",
       " 'get_normed_vectors',\n",
       " 'get_vecattr',\n",
       " 'get_vector',\n",
       " 'has_index_for',\n",
       " 'index2entity',\n",
       " 'index2word',\n",
       " 'index_to_key',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'key_to_index',\n",
       " 'lifecycle_events',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'log_evaluate_word_pairs',\n",
       " 'mapfile_path',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'most_similar_to_given',\n",
       " 'n_similarity',\n",
       " 'next_index',\n",
       " 'norms',\n",
       " 'rank',\n",
       " 'rank_by_centrality',\n",
       " 'relative_cosine_similarity',\n",
       " 'resize_vectors',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'set_vecattr',\n",
       " 'similar_by_key',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'similarity_unseen_docs',\n",
       " 'sort_by_descending_frequency',\n",
       " 'unit_normalize_all',\n",
       " 'vector_size',\n",
       " 'vectors',\n",
       " 'vectors_for_all',\n",
       " 'vectors_norm',\n",
       " 'vocab',\n",
       " 'wmdistance',\n",
       " 'word_vec',\n",
       " 'words_closer_than']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the properties and methods\n",
    "dir(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhguzmDAW-0n"
   },
   "outputs": [],
   "source": [
    "print(f'The dictionary contains {len( glove.key_to_index.keys())} items.' )\n",
    "list(glove.key_to_index.keys())[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FQk8vBxwc3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0QK-KDgwcz_"
   },
   "source": [
    "# Explore the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYV074NVWcWi"
   },
   "outputs": [],
   "source": [
    "# print 10 words at random\n",
    "for idx in np.random.randint(0,len(glove.key_to_index),10):\n",
    "  print(f'Index {idx:>6} is \"{glove.index_to_key[idx]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBZgKWuDzdkY"
   },
   "outputs": [],
   "source": [
    "# distribution of token character lengths\n",
    "token_lengths = np.zeros(len( glove.key_to_index.keys()),dtype=int)\n",
    "for idx,word in enumerate( glove.key_to_index.keys() ):\n",
    "  token_lengths[idx] = len(word)\n",
    "\n",
    "# counts for the bar plot\n",
    "uniqVals,uniqCounts = np.unique(token_lengths,return_counts=True)\n",
    "\n",
    "\n",
    "# visualize the distribution of lengths\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(uniqVals,np.log(uniqCounts),width=uniqVals[1]-uniqVals[0],facecolor=[.9,.7,.9],edgecolor='k')\n",
    "plt.gca().set(xlabel='Word length (num characters)',ylabel='Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuEsbKyWwfHX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVhblyyOwfCK"
   },
   "source": [
    "# Explore the embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68hqmN0eWcZX"
   },
   "outputs": [],
   "source": [
    "# size of the embeddings matrix\n",
    "print(f'The embeddings matrix is {glove.vectors.shape}')\n",
    "\n",
    "print(f'The word \"apple\" has index #{glove.key_to_index[\"apple\"]}')\n",
    "\n",
    "# can also access it this way:\n",
    "glove.get_index('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jxiXdr-dpIv"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(glove.vectors.T,vmin=-1,vmax=1,aspect='auto')\n",
    "plt.gca().set(ylabel='Dimension',xlabel='Word index',title='Embeddings matrix')\n",
    "plt.colorbar(pad=.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzUIGrphv4gA"
   },
   "outputs": [],
   "source": [
    "# mean and std across each embedding dim\n",
    "emb_mean = glove.vectors.mean(axis=1)\n",
    "emb_std  = glove.vectors.std(axis=1)\n",
    "\n",
    "\n",
    "# seaborn has nice visualization routines\n",
    "import seaborn as sns\n",
    "import pandas as pd # though seaborn only works on pandas dataframes :/\n",
    "\n",
    "df = pd.DataFrame(np.vstack((emb_mean,emb_std)).T,columns=['Mean','std'])\n",
    "\n",
    "sns.jointplot(x='Mean',y='std',data=df,alpha=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV-YabMNWcTl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3Bb8KAsWcQf"
   },
   "source": [
    "# Explore individual embeddings vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQsjvRxtZO1T"
   },
   "outputs": [],
   "source": [
    "# pick a word\n",
    "word = 'banana'\n",
    "\n",
    "# get its index in the embeddings matrix\n",
    "wordidx = glove.key_to_index[word]\n",
    "\n",
    "# get the embedding vector\n",
    "thisWordVector = glove.vectors[wordidx,:]\n",
    "\n",
    "# inspect the vector\n",
    "print(f'The embedding vector for \"{word}\" is\\n {thisWordVector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmXGYRRi3RdZ"
   },
   "outputs": [],
   "source": [
    "# even easier ;)\n",
    "thisWordVector = glove[word]\n",
    "\n",
    "print(f'The embedding vector for \"{word}\" is\\n {thisWordVector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOQl-RVFfBOr"
   },
   "outputs": [],
   "source": [
    "# visualize it\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(glove.vectors[wordidx,:],'ks',markersize=10,markerfacecolor=[.7,.7,.9])\n",
    "\n",
    "plt.xlabel('Dimension')\n",
    "plt.title(f'Embedding vector for \"{word}\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLhZxCHLfCJs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edzBOFigfCG9"
   },
   "source": [
    "# Relationships across embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVLhs2G4WcNq"
   },
   "outputs": [],
   "source": [
    "# pick three words\n",
    "word1 = 'banana'\n",
    "word2 = 'apple'\n",
    "word3 = 'cosmic'\n",
    "\n",
    "\n",
    "# setup the figure subplot geometry\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "gs = GridSpec(2,2)\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "ax1 = fig.add_subplot(gs[1,0])\n",
    "ax2 = fig.add_subplot(gs[1,1])\n",
    "\n",
    "# plot the embeddings by dimension\n",
    "for idx,word in enumerate([word1,word2,word3]):\n",
    "  ax0.plot(glove[word],'s-',label=word)\n",
    "\n",
    "ax0.set(xlabel='Dimension',title='Embeddings',xlim=[-1,glove.vectors.shape[1]+1])\n",
    "ax0.legend()\n",
    "\n",
    "\n",
    "# plot the embeddings by each other\n",
    "cossim = glove.similarity(word1,word2)\n",
    "ax1.plot(glove[word1],glove[word2],'ko',markerfacecolor=[.9,.7,.7])\n",
    "ax1.set(xlabel=word1,ylabel=word2,title=f'Cosine similarity = {cossim:.3f}')\n",
    "\n",
    "cossim = glove.similarity(word1,word3)\n",
    "ax2.plot(glove[word1],glove[word3],'ko',markerfacecolor=[.7,.9,.7])\n",
    "ax2.set(xlabel=word1,ylabel=word3,title=f'Cosine similarity = {cossim:.3f}')\n",
    "\n",
    "# final touches\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wQRZJbeWcKz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ3-CHMDsx-b"
   },
   "source": [
    "# Methods to identify similar and dissimilar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJFoUIovsx7r"
   },
   "outputs": [],
   "source": [
    "# most similar words (\"similar\" is high cosine similarity)\n",
    "glove.most_similar('fashion',topn=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCm3OfJJsx40"
   },
   "outputs": [],
   "source": [
    "# One these things is not like the others...\n",
    "lists = [ [ 'apple','banana','pirate','peach' ],\n",
    "          [ 'apple','banana','peach','kiwi','starfruit' ],\n",
    "          [ 'apple','banana','pirate','peach','kiwi','starfruit' ],\n",
    "          [ 'apple','banana','orange','kiwi' ]\n",
    "        ]\n",
    "\n",
    "for l in lists:\n",
    "  print(f'In the word list {l}:')\n",
    "  print(f'  The most similar word is \"{glove.most_similar(l,topn=1)[0][0]}\"')\n",
    "  print(f'  and the non-matching word is \"{glove.doesnt_match(l)}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XqLJMk5sxzi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNdrMjnXI6oEIuDoPx1a90O",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env_LLM",
   "language": "python",
   "name": ".env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
